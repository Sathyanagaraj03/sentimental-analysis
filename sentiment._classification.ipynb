{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv(r'C:\\Users\\visha\\OneDrive\\Desktop\\pytorch\\twitter_training.csv')\n",
    "df1= pd.read_csv(r'C:\\Users\\visha\\OneDrive\\Desktop\\pytorch\\twitter_validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['tweet'], df['sentiment'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.3, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = df['sentiment']) \n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(df1['tweet'], df1['sentiment'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.5, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = df1['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id      int64\n",
       "entity       object\n",
       "sentiment    object\n",
       "tweet        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity'] = df['entity'].astype(str)\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "df['tweet'] = df['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id      int64\n",
       "entity       object\n",
       "sentiment    object\n",
       "tweet        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer \n",
    "bert = AutoModel.from_pretrained('bert-base-uncased') \n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4838     Today, a parcel arrived with Amazon and I did ...\n",
       "11069    Tried my luck to find a Xbox series x today no...\n",
       "49145                                       Highest Prize.\n",
       "61887    I’ve been playing this game for seven centurie...\n",
       "1876     back on my dry borderlands too shit tho or i c...\n",
       "                               ...                        \n",
       "45331    @ FredTJoseph hey fred, Comcast has cut the ca...\n",
       "63268    No fucking flipping way... is DHop a rated 103...\n",
       "51937    I've been playing Red Dead Redemption for 2 da...\n",
       "49913    I don't get this. If-Odoi is so greedy. With F...\n",
       "22046    Catching Change @ CSGO pic.wikipedia.org / JzT...\n",
       "Name: tweet, Length: 52277, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.1953e+04, 1.4302e+04, 5.5720e+03, 4.3000e+02, 4.0000e+00,\n",
       "        5.0000e+00, 0.0000e+00, 1.0000e+00, 3.0000e+00, 7.0000e+00]),\n",
       " array([  0. ,  19.8,  39.6,  59.4,  79.2,  99. , 118.8, 138.6, 158.4,\n",
       "        178.2, 198. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lens = [len(str(i).split()) for i in train_text] \n",
    "plt.hist(train_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4838     Today, a parcel arrived with Amazon and I did ...\n",
       "11069    Tried my luck to find a Xbox series x today no...\n",
       "49145                                       Highest Prize.\n",
       "61887    I’ve been playing this game for seven centurie...\n",
       "1876     back on my dry borderlands too shit tho or i c...\n",
       "                               ...                        \n",
       "45331    @ FredTJoseph hey fred, Comcast has cut the ca...\n",
       "63268    No fucking flipping way... is DHop a rated 103...\n",
       "51937    I've been playing Red Dead Redemption for 2 da...\n",
       "49913    I don't get this. If-Odoi is so greedy. With F...\n",
       "22046    Catching Change @ CSGO pic.wikipedia.org / JzT...\n",
       "Name: tweet, Length: 52277, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4838     Today, a parcel arrived with Amazon and I did ...\n",
       "11069    Tried my luck to find a Xbox series x today no...\n",
       "49145                                       Highest Prize.\n",
       "61887    I’ve been playing this game for seven centurie...\n",
       "1876     back on my dry borderlands too shit tho or i c...\n",
       "                               ...                        \n",
       "45331    @ FredTJoseph hey fred, Comcast has cut the ca...\n",
       "63268    No fucking flipping way... is DHop a rated 103...\n",
       "51937    I've been playing Red Dead Redemption for 2 da...\n",
       "49913    I don't get this. If-Odoi is so greedy. With F...\n",
       "22046    Catching Change @ CSGO pic.wikipedia.org / JzT...\n",
       "Name: tweet, Length: 52277, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2023, 2003, 1037, 7099, 2731, 6251, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2178, 7099, 2731, 6251, 1012,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([0, 1])\n",
      "tensor([[  101,  2023,  2003,  1037,  7099, 27354,  6251,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2178,  7099, 27354,  6251,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([0, 1])\n",
      "tensor([[ 101, 2023, 2003, 1037, 7099, 3231, 6251, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2178, 7099, 3231, 6251, 1012,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Set padding length\n",
    "pad_len = 120\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "tokens_train = tokenizer(\n",
    "    train_text,\n",
    "    max_length=pad_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokens_val = tokenizer(\n",
    "    val_text,\n",
    "    max_length=pad_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer(\n",
    "    test_text,\n",
    "    max_length=pad_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_seq = tokens_train['input_ids']\n",
    "train_mask = tokens_train['attention_mask']\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "val_seq = tokens_val['input_ids']\n",
    "val_mask = tokens_val['attention_mask']\n",
    "val_y = torch.tensor(val_labels)\n",
    "\n",
    "test_seq = tokens_test['input_ids']\n",
    "test_mask = tokens_test['attention_mask']\n",
    "test_y = torch.tensor(test_labels)\n",
    "\n",
    "print(train_seq)\n",
    "print(train_mask)\n",
    "print(train_y)\n",
    "print(val_seq)\n",
    "print(val_mask)\n",
    "print(val_y)\n",
    "print(test_seq)\n",
    "print(test_mask)\n",
    "print(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Freeze the pretrained layers\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define new layers\n",
    "class BERT_architecture(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_architecture, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        \n",
    "        # Dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "        # Softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        # Pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Assuming 'model' is an instance of your BERT_architecture class\n",
    "model = BERT_architecture(bert)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the function to train the model\n",
    "def train_model(model, optimizer, train_dataloader, device, cross_entropy_loss):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "\n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update after every 50 batches\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(f'Batch {step} of {len(train_dataloader)}.')\n",
    "\n",
    "        # Push the batch to GPU\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # Compute the loss between actual and predicted values\n",
    "        loss = cross_entropy_loss(preds, labels)\n",
    "\n",
    "        # Add to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients issue\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Model predictions are stored on GPU, so push to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # Append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # Compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Predictions are in the form of (no. of batches, size of batch, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # Return the average loss and predictions\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function for evaluating the model\n",
    "def evaluate_model(model, val_dataloader, device, cross_entropy_loss):\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "\n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # Progress update every 50 batches\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(f'Batch {step} of {len(val_dataloader)}.')\n",
    "\n",
    "        # Push the batch to GPU\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # Model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # Compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy_loss(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Detach and move predictions to CPU\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # Reshape the predictions in the form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available and choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Alternatively, specify a specific GPU device\n",
    "# device = torch.device(\"cuda:0\")  # Use GPU 0\n",
    "\n",
    "# Example usage with a tensor\n",
    "tensor = torch.tensor([1, 2, 3]).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# get predictions for test data \n",
    "with torch.no_grad(): \n",
    " preds = model(test_seq.to(device), test_mask.to(device)) \n",
    " preds = preds.detach().cpu().numpy() \n",
    "\t\n",
    "\n",
    " pred = np.argmax(preds, axis = 1) \n",
    " print(classification_report(test_y, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming 'model', 'test_seq', 'test_mask', 'test_y', and 'device' are defined\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# Convert predictions to labels\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_y, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
